I started off with trying to implement DQN, because it was mentioned and PPO was already implemented. Spent way too much time trying to use RLlib for it, could not make it work so I sswitched to using stable baselines. So using DQN with stable baselines I trained an agent a bunch of times but it did not do good whatsoever. Then I started trying the already implemented PPO algorithm with rllib, but did not have much success, beceause I did not get the hyperparameters (I still do not). Went back and forth with DQN and PPO trying different hyperparameters that I found on different papaers such as:
 - https://github.com/jkterry1/parameter-sharing-paper?tab=readme-ov-file
 - https://github.com/pettingzoopaper/pettingzoopaper
 - https://github.com/DLR-RM/rl-baselines3-zoo

This did not help much, beacuse I was still not understanding the algorithms much so I decided to search for some better explanations. I stumbled upon this channel:
 - https://www.youtube.com/@johnnycode 

This has a pretty good explanation of DQN and PPO, even though I still don't get PPO. I got some understanding. 

I was pretty done at this point trying random parameters and not getting any result so I did not touch the project for a while. After the QnA I decided to finally try building custom parameters and also saw that I wasn't using the example training script properly. At first I tried without the custom parameters and training wasn't getting any better than 7, 8 average; this is with the default parameters too. Later I decided to increase the batch size a tip from Matteo. That improved it a bit was getting average 10 now. Afterwards I startted playing with the grad_clip parameter too and sgd parameter and that seemed to better the training a bit better too, it took less iterations to get better results. Eventually I added custom features like the direction of the nearest zombie, just one for start. Average reward did improve but still not the best. I decided to just add a bunch more custom features. So distance to nearest zombie, average distance to zombies, average position of zombies, position of nearest zombie, position of player. This imrpoved the average reward significantly. I guess PPO likes those parameters more than the ones given by the observation space even though they are almost the same (not sure why). I also increased the size of the hidden layers for the Mlp module from 64 to 128. This helped significantly now I was getting more than 15 average after almost 100 iterations, but still was not the best. With this setup I got, what I think is randomly, a 30 average after 200 iterations and I could not get it again. So I decided to look further into the evaluation and see what the agent is actually doing visually. I noticed that it was hitting always the nearest to him zombies and not the ones that are nearest to the end of the screen, which was the reason why it was loosing the most. So I decided to add another custom feature, which was the position of and distance of the zombie closest to the end of the screen. WIth this setup I managed to get average reward of 40 during training, which was much better now. Afterwards I inspected the agent visually and I could see that it was indeed doing way better and focusing on the zombies that are the closest to the end of the screen. I uploaded the agent to the department computers for the tournament and it is getting an average reward around 30. Not the best for sure but it is something for 2 days of development. The max is around 40 something. 



At first we tried just using the same custom features for the multi agent setup and same PPO parameters. Our observations from that is - we got a very similar average reward even higher because we had two agents after all and the total average score seemed to be more consistent. But technically it wasn't like the agents were really cooperating. They were just learning the same strategy to shoot at the zombie closest to the bottom of the screen and they were spraying their arrows a lot. While this might not look like real cooperation and might not be the 'best' strategy it works for the setup that we were given and it seems to be quite optimal for the amount of zombies that appear. There is times when the agents miss a zombie or two and then loose the game but it happens rarely at least from the gameplay that we saw. 
For the multi agent I decided to remove the parameter about the zombie that is the closest to the bottom of the screen because both of the agents started shooting the same target always.